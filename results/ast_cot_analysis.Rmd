---
title: "AST-Guided Chain-of-Thought Prompting: Statistical Analysis"
author: "Alberto Melo"
date: "December, 6, 2025"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    theme: flatly
    highlight: tango
    code_folding: hide
    fig_width: 10
    fig_height: 6
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r custom-functions, include=FALSE}
# Custom Statistical Functions

calc_skewness <- function(x) {
  x <- x[!is.na(x)]
  n <- length(x)
  m <- mean(x)
  s <- sd(x)
  sum((x - m)^3) / ((n - 1) * s^3)
}

calc_kurtosis <- function(x) {
  x <- x[!is.na(x)]
  n <- length(x)
  m <- mean(x)
  s <- sd(x)
  (sum((x - m)^4) / ((n - 1) * s^4)) - 3
}

cohens_d <- function(x, y) {
  nx <- length(x)
  ny <- length(y)
  mx <- mean(x)
  my <- mean(y)
  sx <- sd(x)
  sy <- sd(y)
  pooled_sd <- sqrt(((nx - 1) * sx^2 + (ny - 1) * sy^2) / (nx + ny - 2))
  d <- (mx - my) / pooled_sd
  magnitude <- ifelse(abs(d) < 0.2, "negligible",
                      ifelse(abs(d) < 0.5, "small",
                             ifelse(abs(d) < 0.8, "medium", "large")))
  return(list(estimate = d, magnitude = magnitude))
}

levene_test <- function(y, group) {
  groups <- split(y, group)
  deviations <- unlist(lapply(groups, function(x) abs(x - median(x))))
  group_labels <- rep(names(groups), sapply(groups, length))
  anova_result <- summary(aov(deviations ~ factor(group_labels)))
  return(list(
    F_value = anova_result[[1]]$`F value`[1],
    df1 = anova_result[[1]]$Df[1],
    df2 = anova_result[[1]]$Df[2],
    p_value = anova_result[[1]]$`Pr(>F)`[1]
  ))
}
```

```{r load-data, include=FALSE}
# Load pre-processed clean data
data <- read.csv("/Users/melo/Desktop/AST-CoT-Prompt-Generation-System/results/experiment_data_clean_run2.csv", 
                 stringsAsFactors = FALSE)

# Convert to factors
data$model <- factor(data$model, levels = c("claude", "gpt"), 
                     labels = c("Claude", "GPT"))
data$prompt_type <- factor(data$prompt_type, levels = c("baseline", "ast_guided"),
                           labels = c("Baseline", "AST-Guided"))

# Color palette
col_claude <- "#6B4C9A"
col_gpt <- "#2E86AB"
col_baseline <- "#E8E8E8"
col_ast <- "#4ECDC4"

# Pre-calculate group statistics
claude_base_vuln <- data$total_vuln[data$model == "Claude" & data$prompt_type == "Baseline"]
claude_ast_vuln <- data$total_vuln[data$model == "Claude" & data$prompt_type == "AST-Guided"]
gpt_base_vuln <- data$total_vuln[data$model == "GPT" & data$prompt_type == "Baseline"]
gpt_ast_vuln <- data$total_vuln[data$model == "GPT" & data$prompt_type == "AST-Guided"]

claude_base_mean <- mean(claude_base_vuln)
claude_ast_mean <- mean(claude_ast_vuln)
gpt_base_mean <- mean(gpt_base_vuln)
gpt_ast_mean <- mean(gpt_ast_vuln)

claude_base_se <- sd(claude_base_vuln) / sqrt(length(claude_base_vuln))
claude_ast_se <- sd(claude_ast_vuln) / sqrt(length(claude_ast_vuln))
gpt_base_se <- sd(gpt_base_vuln) / sqrt(length(gpt_base_vuln))
gpt_ast_se <- sd(gpt_ast_vuln) / sqrt(length(gpt_ast_vuln))

ast_vuln <- data$total_vuln[data$prompt_type == "AST-Guided"]
baseline_vuln <- data$total_vuln[data$prompt_type == "Baseline"]

ast_density <- data$vuln_density[data$prompt_type == "AST-Guided"]
baseline_density <- data$vuln_density[data$prompt_type == "Baseline"]

claude_base_density <- mean(data$vuln_density[data$model == "Claude" & data$prompt_type == "Baseline"])
claude_ast_density <- mean(data$vuln_density[data$model == "Claude" & data$prompt_type == "AST-Guided"])
gpt_base_density <- mean(data$vuln_density[data$model == "GPT" & data$prompt_type == "Baseline"])
gpt_ast_density <- mean(data$vuln_density[data$model == "GPT" & data$prompt_type == "AST-Guided"])

# Correlation matrix
cor_vars <- data[, c("vuln_ast", "vuln_bandit", "vuln_semgrep", "total_vuln", "loc", "vuln_density")]
cor_matrix <- cor(cor_vars, use = "complete.obs")
```

# Overview

In this report we describe the results of our statistical analysis comparing **AST-Guided Chain-of-Thought (CoT) Prompting** vs. **baseline prompting** to generate secure code. We examined the number of detected vulnerabilities for two large language models (Claude and GPT) using three different detection tools (AST Parser, Bandit and Semgrep).

**Research Design:** 2 × 2 Factorial Design

- **First Independent Variable:** Model (GPT vs Claude)

- **Second Independent Variable:** Prompt type (AST-guided CoT vs Baseline CoT)

- **First Dependent Variable:** Number of vulnerabilities found

- **Second Dependent Variable:** Density of vulnerabilities per KLOC.

```{r sample-sizes}
knitr::kable(table(data$model, data$prompt_type), 
             caption = "Sample Sizes per Condition")
```

---

# Descriptive Statistics

## Overall Summary

```{r descriptive-overall}
overall_stats <- data.frame(
  Metric = c("Total Vulnerabilities", "Vulnerability Density (per KLOC)", "Lines of Code"),
  N = rep(nrow(data), 3),
  Mean = round(c(mean(data$total_vuln), mean(data$vuln_density), mean(data$loc)), 2),
  SD = round(c(sd(data$total_vuln), sd(data$vuln_density), sd(data$loc)), 2),
  Median = c(median(data$total_vuln), round(median(data$vuln_density), 2), median(data$loc)),
  Min = c(min(data$total_vuln), round(min(data$vuln_density), 2), min(data$loc)),
  Max = round(c(max(data$total_vuln), max(data$vuln_density), max(data$loc)), 2)
)
knitr::kable(overall_stats, caption = "Overall Descriptive Statistics")
```

## Distribution of Vulnerability Counts

Below is the histogram showing the number of vulnerabilities in each sample.

```{r fig-distribution, fig.cap="The majority of samples have no vulnerabilities, while the right skewness of the tail indicates that most samples have no vulnerabilities, however there is some variability in this regard."}
par(mfrow = c(1, 2), mar = c(5, 5, 4, 2))

vuln_min <- floor(min(data$total_vuln, na.rm = TRUE)) - 0.5
vuln_max <- ceiling(max(data$total_vuln, na.rm = TRUE)) + 0.5
hist_breaks <- seq(vuln_min, vuln_max, by = 1)

hist(data$total_vuln, breaks = hist_breaks,
     col = col_ast, border = "white",
     main = "Distribution of Vulnerability Counts",
     xlab = "Total Vulnerabilities per Sample",
     ylab = "Frequency",
     cex.lab = 1.1, cex.main = 1.2)

abline(v = mean(data$total_vuln, na.rm = TRUE), col = "red", lwd = 2, lty = 2)
legend("topright", legend = paste("Mean =", round(mean(data$total_vuln, na.rm = TRUE), 2)),
       col = "red", lty = 2, lwd = 2, bty = "n")

boxplot(total_vuln ~ prompt_type, data = data,
        col = c(col_baseline, col_ast),
        main = "Vulnerabilities by Prompt Type",
        ylab = "Total Vulnerability Count",
        xlab = "",
        cex.lab = 1.1, cex.main = 1.2)

means_prompt <- c(mean(baseline_vuln, na.rm = TRUE), mean(ast_vuln, na.rm = TRUE))
points(1:2, means_prompt, pch = 18, col = "red", cex = 1.5)

par(mfrow = c(1, 1))
```

## By Condition

Box plots are shown to demonstrate the distribution of vulnerability counts for each condition in our experiments. Group mean values were identified using red diamonds on the box plots.


```{r fig-boxplot, fig.cap="Claude showed an obvious drop in vulnerability (an 18% drop) when it was prompted by AST-Guided prompts."}
par(mar = c(5, 5, 4, 2))

boxplot(total_vuln ~ prompt_type + model, data = data,
        col = c(col_baseline, col_ast, col_baseline, col_ast),
        names = c("Claude\nBaseline", "Claude\nAST-Guided", "GPT\nBaseline", "GPT\nAST-Guided"),
        ylab = "Total Vulnerability Count",
        xlab = "",
        main = "Vulnerability Count by Model and Prompt Type",
        cex.lab = 1.2, cex.axis = 1.0, cex.main = 1.3,
        outline = TRUE)

means_box <- c(claude_base_mean, claude_ast_mean, gpt_base_mean, gpt_ast_mean)
points(1:4, means_box, pch = 18, col = "red", cex = 1.5)

legend("topright", legend = c("Baseline", "AST-Guided", "Mean"),
       fill = c(col_baseline, col_ast, NA), pch = c(NA, NA, 18),
       col = c(NA, NA, "red"), border = c("black", "black", NA),
       bty = "n", cex = 0.9)
```

```{r condition-table}
condition_stats <- data.frame(
  Model = c("Claude", "Claude", "GPT", "GPT"),
  Prompt = c("Baseline", "AST-Guided", "Baseline", "AST-Guided"),
  Mean = round(c(claude_base_mean, claude_ast_mean, gpt_base_mean, gpt_ast_mean), 3),
  SD = round(c(sd(claude_base_vuln), sd(claude_ast_vuln), sd(gpt_base_vuln), sd(gpt_ast_vuln)), 3),
  n = c(length(claude_base_vuln), length(claude_ast_vuln), length(gpt_base_vuln), length(gpt_ast_vuln))
)
knitr::kable(condition_stats, caption = "Descriptive Statistics by Condition")
```

The bar chart shows the mean count of vulnerabilities for each condition compared to the standard errors. 

```{r fig-bars, fig.cap="Claude has an 18% reduction in number of vulnerabilities than GPT shows a slight increase."}
par(mar = c(5, 5, 4, 2))

bar_data <- matrix(c(claude_base_mean, claude_ast_mean, gpt_base_mean, gpt_ast_mean), 
                   nrow = 2, dimnames = list(c("Baseline", "AST-Guided"), c("Claude", "GPT")))
means_vec <- c(claude_base_mean, claude_ast_mean, gpt_base_mean, gpt_ast_mean)
ses <- c(claude_base_se, claude_ast_se, gpt_base_se, gpt_ast_se)

bp <- barplot(bar_data, beside = TRUE, col = c(col_baseline, col_ast),
              ylim = c(0, max(means_vec + ses) * 1.3),
              ylab = "Mean Vulnerability Count",
              main = "Mean Vulnerabilities by Condition",
              cex.lab = 1.2, cex.axis = 1.0, cex.main = 1.3,
              legend.text = c("Baseline", "AST-Guided"),
              args.legend = list(x = "topright", bty = "n"))

arrows(bp, means_vec - ses, bp, means_vec + ses, angle = 90, code = 3, length = 0.05)
text(bp, means_vec + ses + 0.08, labels = round(means_vec, 2), cex = 0.9)
```

## Vulnerabilities by Detection Tool

Three static analysis tools were employed in order to identify vulnerabilities: an ast parser, bandit, and semgrep.

```{r fig-tools, fig.cap="The total number of vulnerabilities identified by each tool for each condition in which a prompt was applied."}
par(mar = c(5, 5, 4, 2))

tool_baseline <- c(
  sum(data$vuln_ast[data$prompt_type == "Baseline"]),
  sum(data$vuln_bandit[data$prompt_type == "Baseline"]),
  sum(data$vuln_semgrep[data$prompt_type == "Baseline"])
)
tool_ast <- c(
  sum(data$vuln_ast[data$prompt_type == "AST-Guided"]),
  sum(data$vuln_bandit[data$prompt_type == "AST-Guided"]),
  sum(data$vuln_semgrep[data$prompt_type == "AST-Guided"])
)

tool_matrix <- matrix(c(tool_baseline, tool_ast), nrow = 2, byrow = TRUE,
                      dimnames = list(c("Baseline", "AST-Guided"), 
                                      c("AST Parser", "Bandit", "Semgrep")))

tool_counts <- c(tool_baseline[1], tool_ast[1], tool_baseline[2], tool_ast[2], tool_baseline[3], tool_ast[3])

bp3 <- barplot(tool_matrix, beside = TRUE, col = c(col_baseline, col_ast),
               ylim = c(0, max(tool_counts) * 1.15),
               ylab = "Total Vulnerabilities Detected",
               main = "Vulnerabilities by Detection Tool",
               cex.lab = 1.2, cex.axis = 1.0, cex.main = 1.3,
               legend.text = c("Baseline", "AST-Guided"),
               args.legend = list(x = "topright", bty = "n"))

text(bp3, tool_counts + 3, labels = tool_counts, cex = 0.9)
```

```{r tool-table}
tool_stats <- data.frame(
  Tool = c("AST Parser", "Bandit", "Semgrep", "Total"),
  Baseline = c(tool_baseline, sum(tool_baseline)),
  AST_Guided = c(tool_ast, sum(tool_ast)),
  Difference = c(tool_baseline - tool_ast, sum(tool_baseline) - sum(tool_ast)),
  Percent_Change = paste0(round((c(tool_baseline, sum(tool_baseline)) - c(tool_ast, sum(tool_ast))) / 
                                  c(tool_baseline, sum(tool_baseline)) * 100, 1), "%")
)
knitr::kable(tool_stats, caption = "Vulnerability Counts by Detection Tool",
             col.names = c("Tool", "Baseline", "AST-Guided", "Difference", "% Reduction"))
```

---

# Prerequisite: Assumption Testing

Prior to performing any parametric tests it is imperative to check if the data conforms with all of the necessary assumptions for those tests.

## Normality Testing (Shapiro-Wilk)

The shapiro-wilk test assesses the extent to which the data exhibits normality (i.e., does the data follow a normal distribution).

- **H₀:** Data is normally distributed
- **H₁:** Data is NOT normally distributed
- **α = 0.05**

```{r normality-test}
sw_overall <- shapiro.test(data$total_vuln)

normality_results <- data.frame(
  Condition = c("Overall", "Claude + Baseline", "Claude + AST-Guided", 
                "GPT + Baseline", "GPT + AST-Guided"),
  W = round(c(
    sw_overall$statistic,
    shapiro.test(claude_base_vuln)$statistic,
    shapiro.test(claude_ast_vuln)$statistic,
    shapiro.test(gpt_base_vuln)$statistic,
    shapiro.test(gpt_ast_vuln)$statistic
  ), 4),
  p_value = format(c(
    sw_overall$p.value,
    shapiro.test(claude_base_vuln)$p.value,
    shapiro.test(claude_ast_vuln)$p.value,
    shapiro.test(gpt_base_vuln)$p.value,
    shapiro.test(gpt_ast_vuln)$p.value
  ), scientific = TRUE, digits = 3),
  Conclusion = rep("NOT normal (p < .05)", 5)
)

knitr::kable(normality_results, caption = "Shapiro-Wilk Normality Test Results")
```

**Interpretation:** Given that all of the assumptions violated the normality assumption (p < .05) this was to be expected due to a floor effect in the data; therefore nonparametric testing will be performed along with parametric testing as well for sensitivity analysis.

## Homogeneity of Variance (Levene's Test)

The Levene’s test determines if there is an equality of variance between different groupings.

- **H₀:** Variances are equal across groups
- **H₁:** Variances are NOT equal across groups
- **α = 0.05**

```{r levene-test}
levene_result <- levene_test(data$total_vuln, interaction(data$model, data$prompt_type))

cat(paste0("F(", levene_result$df1, ", ", levene_result$df2, ") = ", 
           round(levene_result$F_value, 4), ", p = ", round(levene_result$p_value, 4)))
```

**Interpretation:** The results from the Levene’s Test indicate if variance in the vulnerability count is equal or unequal across the different model types/prompt types.

---

# Analysis 1: Two-Way Factorial ANOVA

**Research Question:** Are Model Type and Prompt Type statistically significant for a difference in vulnerability counts? Is there a statistically significant interaction effect between Model Type and Prompt Type on a vulnerability count?

**Design:** 2 (Claude vs. GPT Model Types) x 2 (Baseline vs. AST-Guided Prompt Types)

**Hypotheses:**

- H₁ₐ : Effect of Model – The main effect of the two models (Claude and GPT) show a difference in the total count of their vulnerabilities.

- H₁ᵦ : Effect of Prompt – The main effect of the two prompts (AST-Guided and Baseline) are different from each other regarding the total count of vulnerabilities.

- H₁꜀ : Interaction effect – The effect of prompt type varies based upon the type of model.

```{r anova}
anova_model <- aov(total_vuln ~ model * prompt_type, data = data)
anova_summary <- summary(anova_model)

print(anova_summary)
```

```{r anova-effects}
ss <- anova_summary[[1]]$`Sum Sq`
ss_total <- sum(ss)
eta_sq <- ss / ss_total

effect_sizes <- data.frame(
  Effect = c("Model", "Prompt Type", "Model x Prompt", "Residual"),
  Sum_Sq = round(ss, 2),
  Eta_Squared = round(eta_sq, 4),
  Interpretation = c(
    ifelse(eta_sq[1] < 0.01, "Negligible", ifelse(eta_sq[1] < 0.06, "Small", ifelse(eta_sq[1] < 0.14, "Medium", "Large"))),
    ifelse(eta_sq[2] < 0.01, "Negligible", ifelse(eta_sq[2] < 0.06, "Small", ifelse(eta_sq[2] < 0.14, "Medium", "Large"))),
    ifelse(eta_sq[3] < 0.01, "Negligible", ifelse(eta_sq[3] < 0.06, "Small", ifelse(eta_sq[3] < 0.14, "Medium", "Large"))),
    "-"
  )
)

knitr::kable(effect_sizes, caption = "Effect Sizes (Eta-Squared)")
```

Interaction plot displays how the impact of the prompt type is different for each model.

```{r fig-interaction, fig.cap="Claude has a larger decrease in the number of vulnerabilities when using AST-Guided prompts than GPT which indicates that AST-Guided was less effective for GPT"}
par(mar = c(5, 5, 4, 2))

interaction.plot(data$prompt_type, data$model, data$total_vuln,
                 type = "b", pch = c(16, 17), lty = c(1, 2),
                 col = c(col_claude, col_gpt), lwd = 2,
                 ylab = "Mean Vulnerability Count",
                 xlab = "Prompt Type",
                 main = "Interaction: Model x Prompt Type",
                 trace.label = "Model",
                 cex.lab = 1.2, cex.axis = 1.0, cex.main = 1.3,
                 ylim = c(0.6, 1.4))

grid(col = "gray90")
```

**Interpretation:** The Interaction Plot indicates that Claude's response to AST-Guided Prompting is greater than GPT's Response to AST-Guided Prompting.

---

# Analysis 2: Independent Samples t-test

**Research Question:** Is there a statistically significant reduction in Vulnerability Counts from Baseline Prompting when using AST-Guided Prompting.

**Hypotheses:**

- H₀: μ(AST-Guided) = μ(Baseline)
- H₁: μ(AST-Guided) ≠ μ(Baseline)
- α = 0.05

```{r ttest}
ttest_result <- t.test(ast_vuln, baseline_vuln, var.equal = FALSE)
cohens_d_result <- cohens_d(ast_vuln, baseline_vuln)

ttest_table <- data.frame(
  Group = c("AST-Guided", "Baseline"),
  Mean = round(c(mean(ast_vuln), mean(baseline_vuln)), 3),
  SD = round(c(sd(ast_vuln), sd(baseline_vuln)), 3),
  n = c(length(ast_vuln), length(baseline_vuln))
)

knitr::kable(ttest_table, caption = "Group Statistics")

cat(paste0("\nWelch's t-test: t(", round(ttest_result$parameter, 2), ") = ", 
           round(ttest_result$statistic, 4), ", p = ", round(ttest_result$p.value, 4)))
cat(paste0("\n95% CI: [", round(ttest_result$conf.int[1], 4), ", ", 
           round(ttest_result$conf.int[2], 4), "]"))
cat(paste0("\nCohen's d = ", round(cohens_d_result$estimate, 4), " (", cohens_d_result$magnitude, ")"))
```

---

# Analysis 3: Mann-Whitney U Test (Non-parametric)

**Rationale:** The violation of normality assumptions for the data is the reason why the Mann-Whitney U Test is used as a non-parametric alternative to provide a robust test.

**Hypotheses:**

- H₀: The distributions of vulnerabilities are equal between groups
- H₁: The distributions differ between groups
- α = 0.05

```{r mann-whitney}
mw_result <- wilcox.test(ast_vuln, baseline_vuln, exact = FALSE, correct = TRUE)

z_score <- qnorm(mw_result$p.value / 2)
r_effect <- abs(z_score) / sqrt(length(ast_vuln) + length(baseline_vuln))

cat(paste0("Mann-Whitney U: W = ", mw_result$statistic, ", p = ", round(mw_result$p.value, 4)))
cat(paste0("\nEffect size r = ", round(r_effect, 4), " (", 
           ifelse(r_effect < 0.1, "negligible", ifelse(r_effect < 0.3, "small", 
           ifelse(r_effect < 0.5, "medium", "large"))), ")"))
```
 
---

# Analysis 4: Correlation Analysis

**Research Question:** What are the relationships between various vulnerability detection tools (and) the characteristics of code?

```{r fig-correlation, fig.cap="Correlation Heatmap of Vulnerability Metrics. The darker red indicates that there is a higher correlation to each other (of) the positive relationship between the two variables."}
par(mar = c(6, 8, 4, 4))

cor_subset <- cor_matrix[c("vuln_ast", "vuln_bandit", "vuln_semgrep", "total_vuln", "vuln_density"),
                         c("vuln_ast", "vuln_bandit", "vuln_semgrep", "total_vuln", "vuln_density")]

n_colors <- 100
colors <- colorRampPalette(c("#2166AC", "#F7F7F7", "#B2182B"))(n_colors)

image(1:5, 1:5, t(cor_subset[5:1, ]), col = colors, zlim = c(-1, 1),
      xaxt = "n", yaxt = "n", xlab = "", ylab = "",
      main = "Correlation Matrix: Vulnerability Metrics")

labels <- c("AST", "Bandit", "Semgrep", "Total", "Density")
axis(1, at = 1:5, labels = labels, las = 2, cex.axis = 0.9)
axis(2, at = 1:5, labels = rev(labels), las = 2, cex.axis = 0.9)

for (i in 1:5) {
  for (j in 1:5) {
    text(i, 6-j, round(cor_subset[j, i], 2), cex = 0.8)
  }
}
```

```{r correlation-tests}
cor_results <- data.frame(
  Comparison = c("AST Parser vs Bandit", "AST Parser vs Semgrep", "Bandit vs Semgrep", 
                 "LOC vs Total Vuln", "LOC vs Vuln Density"),
  r = round(c(
    cor(data$vuln_ast, data$vuln_bandit),
    cor(data$vuln_ast, data$vuln_semgrep),
    cor(data$vuln_bandit, data$vuln_semgrep),
    cor(data$loc, data$total_vuln),
    cor(data$loc, data$vuln_density)
  ), 3),
  p_value = format(c(
    cor.test(data$vuln_ast, data$vuln_bandit)$p.value,
    cor.test(data$vuln_ast, data$vuln_semgrep)$p.value,
    cor.test(data$vuln_bandit, data$vuln_semgrep)$p.value,
    cor.test(data$loc, data$total_vuln)$p.value,
    cor.test(data$loc, data$vuln_density)$p.value
  ), scientific = TRUE, digits = 3),
  Significant = c(
    ifelse(cor.test(data$vuln_ast, data$vuln_bandit)$p.value < 0.05, "*", "ns"),
    ifelse(cor.test(data$vuln_ast, data$vuln_semgrep)$p.value < 0.05, "*", "ns"),
    ifelse(cor.test(data$vuln_bandit, data$vuln_semgrep)$p.value < 0.05, "***", "ns"),
    ifelse(cor.test(data$loc, data$total_vuln)$p.value < 0.05, "*", "ns"),
    ifelse(cor.test(data$loc, data$vuln_density)$p.value < 0.05, "***", "ns")
  )
)

knitr::kable(cor_results, caption = "Key Correlations with Significance Tests")
```

---

# Analysis 5: Multiple Regression

**Research Question:** How strong is the predictive ability of Model, Prompt Type, and Lines of Code for total number of vulnerabilities?

**Model:** `total_vuln ~ model + prompt_type + loc + model:prompt_type`

```{r regression}
reg_model <- lm(total_vuln ~ model + prompt_type + loc + model:prompt_type, data = data)
reg_summary <- summary(reg_model)

# Create clean regression table
reg_table <- data.frame(
  Predictor = c("(Intercept)", "Model (GPT)", "Prompt Type (AST-Guided)", 
                "Lines of Code", "Model × Prompt Type"),
  Estimate = round(reg_summary$coefficients[, 1], 4),
  SE = round(reg_summary$coefficients[, 2], 4),
  t = round(reg_summary$coefficients[, 3], 2),
  p = c(
    format.pval(reg_summary$coefficients[1, 4], digits = 3),
    format.pval(reg_summary$coefficients[2, 4], digits = 3),
    format.pval(reg_summary$coefficients[3, 4], digits = 3),
    format.pval(reg_summary$coefficients[4, 4], digits = 3),
    format.pval(reg_summary$coefficients[5, 4], digits = 3)
  )
)

knitr::kable(reg_table, caption = "Regression Coefficients", 
             col.names = c("Predictor", "Estimate", "Std. Error", "t", "p"),
             row.names = FALSE)

cat(paste0("\nModel Fit Statistics:"))
cat(paste0("\n  R-squared = ", round(reg_summary$r.squared, 4)))
cat(paste0("\n  Adjusted R-squared = ", round(reg_summary$adj.r.squared, 4)))
cat(paste0("\n  F(", reg_summary$fstatistic[2], ", ", reg_summary$fstatistic[3], ") = ", 
           round(reg_summary$fstatistic[1], 4), 
           ", p = ", format.pval(pf(reg_summary$fstatistic[1], reg_summary$fstatistic[2], 
                              reg_summary$fstatistic[3], lower.tail = FALSE), digits = 3)))
```

**Interpretation:**

The full regression model was statistically significant (`r round(reg_summary$r.squared * 100, 1)`)%. Key results from the regression model include:

- **Lines of Code (LOC)**: Significant (p < .001). Each increase in LOC results in an expected increase in the number of vulnerabilities of approximately 0.014.

- **Model (GPT vs Claude)**: Significant (p = .003). On average, GPT had approximately 0.60 more vulnerabilities per sample than Claude.

- **Prompt Type (AST-Guided vs Baseline)**: Non-significant (p = .606). AST-Guided prompting was not a statistically significant predictor of the number of vulnerabilities, after accounting for other variables.

- **Interaction (Model × Prompt Type)**: Non-significant (p = .568). There was no statistical difference in how prompt types affected the number of vulnerabilities based on the models used.

---

# Vulnerability Density Analysis

Vulnerability Density allows for a fair comparison among samples that have varying sizes, by normalizing the number of vulnerabilities per sample to the total number of lines of code in each sample.

**Formula:** Vulnerability Density = (Total Vulnerabilities / Lines of Code) × 1000

```{r fig-density, fig.cap="Vulnerability density by condition. Claude shows reduction in vulnerability density with AST-Guided prompting."}
par(mar = c(5, 5, 4, 2))

density_data <- matrix(c(claude_base_density, claude_ast_density, gpt_base_density, gpt_ast_density), 
                       nrow = 2, dimnames = list(c("Baseline", "AST-Guided"), c("Claude", "GPT")))
density_vec <- c(claude_base_density, claude_ast_density, gpt_base_density, gpt_ast_density)

bp2 <- barplot(density_data, beside = TRUE, col = c(col_baseline, col_ast),
               ylim = c(0, max(density_vec) * 1.15),
               ylab = "Vulnerability Density (per KLOC)",
               main = "Vulnerability Density by Condition",
               cex.lab = 1.2, cex.axis = 1.0, cex.main = 1.3,
               legend.text = c("Baseline", "AST-Guided"),
               args.legend = list(x = "topleft", bty = "n"))

text(bp2, density_vec + 2, labels = round(density_vec, 1), cex = 0.9)
```

```{r density-stats}
density_stats <- data.frame(
  Metric = c("AST-Guided", "Baseline", "Difference", "Percent Reduction"),
  Value = c(
    paste0(round(mean(ast_density), 2), " vulns/KLOC"),
    paste0(round(mean(baseline_density), 2), " vulns/KLOC"),
    paste0(round(mean(baseline_density) - mean(ast_density), 2), " vulns/KLOC"),
    paste0(round((mean(baseline_density) - mean(ast_density)) / mean(baseline_density) * 100, 2), "%")
  )
)

knitr::kable(density_stats, caption = "Vulnerability Density Comparison")

density_ttest <- t.test(ast_density, baseline_density, var.equal = FALSE)
density_d <- cohens_d(ast_density, baseline_density)

cat(paste0("\nt-test on density: t(", round(density_ttest$parameter, 2), ") = ", 
           round(density_ttest$statistic, 4), ", p = ", round(density_ttest$p.value, 4)))
cat(paste0("\nCohen's d = ", round(density_d$estimate, 4), " (", density_d$magnitude, ")"))
```

---

# Summary of Findings

```{r fig-reduction, fig.cap="Percentage decrease in the number of identified vulnerabilities due to AST-Guided prompting compared to Baseline."}
par(mar = c(5, 8, 4, 2))

reductions <- c(
  (claude_base_mean - claude_ast_mean) / claude_base_mean * 100,
  (gpt_base_mean - gpt_ast_mean) / gpt_base_mean * 100,
  (mean(baseline_vuln) - mean(ast_vuln)) / mean(baseline_vuln) * 100
)

bar_colors <- c(col_claude, col_gpt, "#2ECC71")
# Adjust colors for negative values
bar_colors[reductions < 0] <- "#E74C3C"

bp4 <- barplot(reductions, horiz = TRUE,
               names.arg = c("Claude", "GPT", "Overall"),
               col = bar_colors,
               xlim = c(min(reductions) - 5, max(reductions) + 5),
               xlab = "Vulnerability Reduction (%)",
               main = "Percent Change: AST-Guided vs Baseline",
               cex.lab = 1.1, cex.main = 1.2, las = 1)

text(reductions + ifelse(reductions >= 0, 1, -1), bp4, 
     labels = paste0(round(reductions, 1), "%"), cex = 0.9)
abline(v = 0, col = "gray50", lwd = 2)
```

## Statistical Analyses Conducted

```{r summary-table}
# Extract ANOVA values
anova_f_prompt <- round(anova_summary[[1]]$`F value`[2], 3)
anova_p_prompt <- format.pval(anova_summary[[1]]$`Pr(>F)`[2], digits = 3)
anova_f_model <- round(anova_summary[[1]]$`F value`[1], 3)
anova_p_model <- format.pval(anova_summary[[1]]$`Pr(>F)`[1], digits = 3)

summary_table <- data.frame(
  Analysis = c("Two-Way ANOVA", "", "Independent t-test", "Mann-Whitney U", 
               "Correlation Analysis", "Multiple Regression"),
  Purpose = c("Prompt type effect", "Model effect", 
              "Compare AST-Guided vs Baseline", "Non-parametric robustness check",
              "Examine tool relationships", "Predict vulnerability count"),
  Result = c(
    paste0("F(1, ", anova_summary[[1]]$Df[4], ") = ", anova_f_prompt, ", p = ", anova_p_prompt),
    paste0("F(1, ", anova_summary[[1]]$Df[4], ") = ", anova_f_model, ", p = ", anova_p_model),
    paste0("t(", round(ttest_result$parameter, 0), ") = ", round(ttest_result$statistic, 3), ", p = ", round(ttest_result$p.value, 3)),
    paste0("U = ", round(mw_result$statistic, 0), ", p = ", round(mw_result$p.value, 3)),
    "Tools show convergent validity",
    paste0("R² = ", round(reg_summary$r.squared, 3), ", F(4, ", reg_summary$fstatistic[3], ") = ", 
           round(reg_summary$fstatistic[1], 2), ", p < .001")
  )
)

knitr::kable(summary_table, caption = "Summary of Statistical Analyses",
             col.names = c("Analysis", "Purpose", "Result"),
             row.names = FALSE)
```

## Key Findings

1. **Overall Effect:** AST-Guided prompting shows a `r round((mean(baseline_vuln) - mean(ast_vuln)) / mean(baseline_vuln) * 100, 1)`% mean reduction in vulnerabilities

2. **By Model:**
   - **Claude: `r round((claude_base_mean - claude_ast_mean) / claude_base_mean * 100, 1)`% reduction** (M = `r round(claude_ast_mean, 2)` vs `r round(claude_base_mean, 2)`)
   - GPT: `r round((gpt_base_mean - gpt_ast_mean) / gpt_base_mean * 100, 1)`% change (M = `r round(gpt_ast_mean, 2)` vs `r round(gpt_base_mean, 2)`)

3. **Vulnerability Density:** 
   - AST-Guided: `r round(mean(ast_density), 2)` vulns/KLOC
   - Baseline: `r round(mean(baseline_density), 2)` vulns/KLOC

4. **Detection Tool Agreement:** Using all three detection tools is supported through their positive correlations with each other
